{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling for Student Success\n",
    "\n",
    "This notebook builds machine learning models to predict student outcomes using the engineered features from the OULAD dataset. We'll focus on early warning systems that can identify at-risk students within the first 3-4 weeks of a course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the engineered features\n",
    "data_dir = Path(\"../data/processed\")\n",
    "df = pd.read_csv(data_dir / \"student_features_engineered.csv\")\n",
    "\n",
    "print(f\"Loaded {len(df)} records with {len(df.columns)} columns\")\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df['final_result'].value_counts())\n",
    "print(f\"\\nSuccess rate: {(df['final_result'].isin(['Pass', 'Distinction']).sum() / len(df) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "Prepare the data for machine learning modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Missing values before preprocessing:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "# Fill missing registration_delay with median\n",
    "df['registration_delay'] = df['registration_delay'].fillna(df['registration_delay'].median())\n",
    "\n",
    "# Fill any remaining missing values with 0 (for early engagement features)\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(\"\\nMissing values after preprocessing:\")\n",
    "print(df.isnull().sum().sum())\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [col for col in df.columns if col not in ['id_student', 'code_module', 'code_presentation', 'final_result']]\n",
    "X = df[feature_columns]\n",
    "y = df['final_result']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {len(feature_columns)}\")\n",
    "print(\"\\nFeature list:\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary Classification: Pass vs Fail\n",
    "\n",
    "First, let's build a binary classifier to predict Pass/Fail, focusing on students who complete the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification dataset (exclude withdrawn students)\n",
    "binary_mask = df['final_result'].isin(['Pass', 'Distinction', 'Fail'])\n",
    "X_binary = X[binary_mask]\n",
    "y_binary = y[binary_mask]\n",
    "\n",
    "# Convert to binary labels (1 = Pass/Distinction, 0 = Fail)\n",
    "y_binary = (y_binary.isin(['Pass', 'Distinction'])).astype(int)\n",
    "\n",
    "print(f\"Binary classification dataset: {len(X_binary)} records\")\n",
    "print(f\"Success rate: {y_binary.mean():.1%}\")\n",
    "print(f\"Class distribution: {y_binary.value_counts().to_dict()}\")\n",
    "\n",
    "# Split the data\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler_bin = StandardScaler()\n",
    "X_train_bin_scaled = scaler_bin.fit_transform(X_train_bin)\n",
    "X_test_bin_scaled = scaler_bin.transform(X_test_bin)\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train_bin)} records\")\n",
    "print(f\"Test set: {len(X_test_bin)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple binary classifiers\n",
    "binary_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "binary_results = {}\n",
    "\n",
    "print(\"=== BINARY CLASSIFICATION RESULTS (Pass/Fail) ===\")\n",
    "print()\n",
    "\n",
    "for name, model in binary_models.items():\n",
    "    # Train the model\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_bin_scaled, y_train_bin)\n",
    "        y_pred = model.predict(X_test_bin_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_bin_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train_bin, y_train_bin)\n",
    "        y_pred = model.predict(X_test_bin)\n",
    "        y_pred_proba = model.predict_proba(X_test_bin)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_bin, y_pred)\n",
    "    precision = precision_score(y_test_bin, y_pred)\n",
    "    recall = recall_score(y_test_bin, y_pred)\n",
    "    f1 = f1_score(y_test_bin, y_pred)\n",
    "    auc = roc_auc_score(y_test_bin, y_pred_proba)\n",
    "    \n",
    "    binary_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    print(f\"  F1-Score:  {f1:.3f}\")\n",
    "    print(f\"  AUC:       {auc:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Find best model\n",
    "best_binary_model = max(binary_results.items(), key=lambda x: x[1]['auc'])\n",
    "print(f\"üèÜ Best binary model: {best_binary_model[0]} (AUC: {best_binary_model[1]['auc']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-class Classification: All Outcomes\n",
    "\n",
    "Now let's build a model to predict all four outcomes: Pass, Distinction, Fail, Withdrawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class classification setup\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"Class mapping:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i}: {class_name}\")\n",
    "\n",
    "# Split the data\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler_multi = StandardScaler()\n",
    "X_train_multi_scaled = scaler_multi.fit_transform(X_train_multi)\n",
    "X_test_multi_scaled = scaler_multi.transform(X_test_multi)\n",
    "\n",
    "print(f\"\\nMulti-class dataset: {len(X)} records\")\n",
    "print(f\"Training set: {len(X_train_multi)} records\")\n",
    "print(f\"Test set: {len(X_test_multi)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multi-class models\n",
    "multi_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, multi_class='ovr'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "multi_results = {}\n",
    "\n",
    "print(\"=== MULTI-CLASS CLASSIFICATION RESULTS ===\")\n",
    "print()\n",
    "\n",
    "for name, model in multi_models.items():\n",
    "    # Train the model\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_multi_scaled, y_train_multi)\n",
    "        y_pred = model.predict(X_test_multi_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_multi_scaled)\n",
    "    else:\n",
    "        model.fit(X_train_multi, y_train_multi)\n",
    "        y_pred = model.predict(X_test_multi)\n",
    "        y_pred_proba = model.predict_proba(X_test_multi)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_multi, y_pred)\n",
    "    precision = precision_score(y_test_multi, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_multi, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_multi, y_pred, average='weighted')\n",
    "    \n",
    "    multi_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    print(f\"  F1-Score:  {f1:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Find best model\n",
    "best_multi_model = max(multi_results.items(), key=lambda x: x[1]['f1'])\n",
    "print(f\"üèÜ Best multi-class model: {best_multi_model[0]} (F1: {best_multi_model[1]['f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for predicting student outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from best models\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"Extract feature importance from trained model\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # For logistic regression, use absolute coefficients\n",
    "        if len(model.coef_.shape) == 1:\n",
    "            coef = model.coef_\n",
    "        else:\n",
    "            coef = np.mean(np.abs(model.coef_), axis=0)\n",
    "        return pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': np.abs(coef)\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Analyze feature importance for binary classification\n",
    "best_binary_name = best_binary_model[0]\n",
    "binary_importance = get_feature_importance(best_binary_model[1]['model'], feature_columns)\n",
    "\n",
    "print(f\"=== FEATURE IMPORTANCE - {best_binary_name} (Binary) ===\")\n",
    "print(\"Top 15 most important features:\")\n",
    "print(binary_importance.head(15))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = binary_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title(f'Top 15 Features - {best_binary_name} (Binary Classification)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze feature importance for multi-class\n",
    "best_multi_name = best_multi_model[0]\n",
    "multi_importance = get_feature_importance(best_multi_model[1]['model'], feature_columns)\n",
    "\n",
    "print(f\"\\n=== FEATURE IMPORTANCE - {best_multi_name} (Multi-class) ===\")\n",
    "print(\"Top 15 most important features:\")\n",
    "print(multi_importance.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Visualization\n",
    "\n",
    "Detailed evaluation of our best models with confusion matrices and ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for binary classification\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Binary classification confusion matrix\n",
    "cm_binary = confusion_matrix(y_test_bin, best_binary_model[1]['predictions'])\n",
    "sns.heatmap(cm_binary, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'], ax=axes[0])\n",
    "axes[0].set_title(f'Binary Classification - {best_binary_name}')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Multi-class confusion matrix\n",
    "cm_multi = confusion_matrix(y_test_multi, best_multi_model[1]['predictions'])\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, ax=axes[1])\n",
    "axes[1].set_title(f'Multi-class Classification - {best_multi_name}')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve for binary classification\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr, tpr, _ = roc_curve(y_test_bin, best_binary_model[1]['probabilities'])\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'{best_binary_name} (AUC = {best_binary_model[1][\"auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Binary Classification (Pass vs Fail)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "print(f\"\\nBinary Classification - {best_binary_name}:\")\n",
    "print(classification_report(y_test_bin, best_binary_model[1]['predictions'], \n",
    "                          target_names=['Fail', 'Pass']))\n",
    "\n",
    "print(f\"\\nMulti-class Classification - {best_multi_name}:\")\n",
    "print(classification_report(y_test_multi, best_multi_model[1]['predictions'], \n",
    "                          target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Early Warning System\n",
    "\n",
    "Create a practical early warning system for identifying at-risk students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create risk assessment function\n",
    "def assess_student_risk(student_data, model, scaler=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Assess risk for a single student or batch of students\n",
    "    \n",
    "    Args:\n",
    "        student_data: DataFrame with student features\n",
    "        model: Trained model\n",
    "        scaler: Scaler for feature normalization (if needed)\n",
    "        threshold: Risk threshold for classification\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with risk assessment\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    if scaler is not None:\n",
    "        features_scaled = scaler.transform(student_data[feature_columns])\n",
    "        risk_proba = model.predict_proba(features_scaled)[:, 1]  # Probability of success\n",
    "    else:\n",
    "        risk_proba = model.predict_proba(student_data[feature_columns])[:, 1]\n",
    "    \n",
    "    # Calculate risk score (1 - probability of success)\n",
    "    risk_score = 1 - risk_proba\n",
    "    \n",
    "    # Assign risk categories\n",
    "    risk_category = pd.cut(risk_score, \n",
    "                          bins=[0, 0.3, 0.6, 1.0], \n",
    "                          labels=['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'student_id': student_data['id_student'] if 'id_student' in student_data else range(len(student_data)),\n",
    "        'success_probability': risk_proba,\n",
    "        'risk_score': risk_score,\n",
    "        'risk_category': risk_category,\n",
    "        'needs_intervention': risk_score > threshold\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the early warning system on test data\n",
    "test_data = pd.concat([X_test_bin.reset_index(drop=True), \n",
    "                       df.loc[X_test_bin.index, ['id_student']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Use best binary model for risk assessment\n",
    "best_model = best_binary_model[1]['model']\n",
    "scaler = scaler_bin if best_binary_name == 'Logistic Regression' else None\n",
    "\n",
    "risk_assessment = assess_student_risk(test_data, best_model, scaler, threshold=0.5)\n",
    "\n",
    "print(\"=== EARLY WARNING SYSTEM RESULTS ===\")\n",
    "print(f\"\\nRisk distribution in test set:\")\n",
    "print(risk_assessment['risk_category'].value_counts())\n",
    "print(f\"\\nStudents needing intervention: {risk_assessment['needs_intervention'].sum()}/{len(risk_assessment)} ({risk_assessment['needs_intervention'].mean():.1%})\")\n",
    "\n",
    "# Show examples of high-risk students\n",
    "print(\"\\nHigh-risk students (sample):\")\n",
    "high_risk = risk_assessment[risk_assessment['risk_category'] == 'High Risk'].head(10)\n",
    "print(high_risk[['student_id', 'success_probability', 'risk_score', 'risk_category']].round(3))\n",
    "\n",
    "# Visualize risk distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "risk_assessment['risk_category'].value_counts().plot(kind='bar')\n",
    "plt.title('Risk Category Distribution')\n",
    "plt.xlabel('Risk Category')\n",
    "plt.ylabel('Number of Students')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(risk_assessment['risk_score'], bins=30, alpha=0.7, color='skyblue')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', label='Intervention Threshold')\n",
    "plt.xlabel('Risk Score')\n",
    "plt.ylabel('Number of Students')\n",
    "plt.title('Risk Score Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Interpretation and Insights\n",
    "\n",
    "Extract actionable insights from our models for educational interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature patterns for at-risk students\n",
    "def analyze_risk_patterns(data, risk_assessment, feature_cols):\n",
    "    \"\"\"\n",
    "    Analyze feature patterns for different risk categories\n",
    "    \"\"\"\n",
    "    # Combine data with risk assessment\n",
    "    analysis_data = pd.concat([data[feature_cols].reset_index(drop=True), \n",
    "                              risk_assessment[['risk_category']].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Calculate means by risk category\n",
    "    risk_patterns = analysis_data.groupby('risk_category')[feature_cols].mean()\n",
    "    \n",
    "    return risk_patterns\n",
    "\n",
    "# Analyze patterns\n",
    "risk_patterns = analyze_risk_patterns(test_data, risk_assessment, feature_columns)\n",
    "\n",
    "print(\"=== RISK PATTERN ANALYSIS ===\")\n",
    "print(\"\\nKey differences between risk categories:\")\n",
    "print(\"\\nTop features showing largest differences:\")\n",
    "\n",
    "# Calculate feature differences between high and low risk\n",
    "if 'High Risk' in risk_patterns.index and 'Low Risk' in risk_patterns.index:\n",
    "    feature_diffs = (risk_patterns.loc['High Risk'] - risk_patterns.loc['Low Risk']).abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 discriminating features:\")\n",
    "    for i, (feature, diff) in enumerate(feature_diffs.head(10).items(), 1):\n",
    "        high_val = risk_patterns.loc['High Risk', feature]\n",
    "        low_val = risk_patterns.loc['Low Risk', feature]\n",
    "        print(f\"{i:2d}. {feature}: High={high_val:.3f}, Low={low_val:.3f}, Diff={diff:.3f}\")\n",
    "\n",
    "# Create actionable insights\n",
    "print(\"\\n=== ACTIONABLE INSIGHTS FOR INTERVENTIONS ===\")\n",
    "print(\"\\n1. EARLY ENGAGEMENT (First 28 days):\")\n",
    "print(\"   - Monitor VLE interaction patterns\")\n",
    "print(\"   - Target students with low click rates\")\n",
    "print(\"   - Focus on engagement consistency\")\n",
    "\n",
    "print(\"\\n2. ASSESSMENT PERFORMANCE (First 70 days):\")\n",
    "print(\"   - Track submission rates closely\")\n",
    "print(\"   - Identify students missing early assessments\")\n",
    "print(\"   - Provide support for low-scoring students\")\n",
    "\n",
    "print(\"\\n3. DEMOGRAPHIC RISK FACTORS:\")\n",
    "print(\"   - Provide additional support for high-risk demographics\")\n",
    "print(\"   - Consider study load adjustments\")\n",
    "print(\"   - Address registration delay issues\")\n",
    "\n",
    "print(\"\\n4. INTERVENTION TIMING:\")\n",
    "print(\"   - Week 2: Initial engagement check\")\n",
    "print(\"   - Week 4: First assessment review\")\n",
    "print(\"   - Week 6: Comprehensive risk assessment\")\n",
    "print(\"   - Week 10: Mid-course intervention\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDED INTERVENTIONS:\")\n",
    "print(\"   - High Risk: Intensive tutoring, study groups, counseling\")\n",
    "print(\"   - Medium Risk: Regular check-ins, peer support, resources\")\n",
    "print(\"   - Low Risk: Self-directed learning, advanced challenges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models and Results\n",
    "\n",
    "Save the trained models and evaluation results for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path(\"../results/models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save best models\n",
    "joblib.dump(best_binary_model[1]['model'], models_dir / \"best_binary_model.pkl\")\n",
    "joblib.dump(best_multi_model[1]['model'], models_dir / \"best_multi_model.pkl\")\n",
    "joblib.dump(scaler_bin, models_dir / \"binary_scaler.pkl\")\n",
    "joblib.dump(scaler_multi, models_dir / \"multi_scaler.pkl\")\n",
    "joblib.dump(label_encoder, models_dir / \"label_encoder.pkl\")\n",
    "\n",
    "# Save feature columns\n",
    "with open(models_dir / \"feature_columns.json\", 'w') as f:\n",
    "    json.dump(feature_columns, f)\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'best_binary_model': {\n",
    "        'name': best_binary_name,\n",
    "        'metrics': {\n",
    "            'accuracy': best_binary_model[1]['accuracy'],\n",
    "            'precision': best_binary_model[1]['precision'],\n",
    "            'recall': best_binary_model[1]['recall'],\n",
    "            'f1': best_binary_model[1]['f1'],\n",
    "            'auc': best_binary_model[1]['auc']\n",
    "        }\n",
    "    },\n",
    "    'best_multi_model': {\n",
    "        'name': best_multi_name,\n",
    "        'metrics': {\n",
    "            'accuracy': best_multi_model[1]['accuracy'],\n",
    "            'precision': best_multi_model[1]['precision'],\n",
    "            'recall': best_multi_model[1]['recall'],\n",
    "            'f1': best_multi_model[1]['f1']\n",
    "        }\n",
    "    },\n",
    "    'feature_importance': binary_importance.to_dict('records'),\n",
    "    'dataset_info': {\n",
    "        'total_records': len(df),\n",
    "        'features': len(feature_columns),\n",
    "        'binary_records': len(X_binary),\n",
    "        'test_size': len(X_test_bin)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(models_dir / \"model_metadata.json\", 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "# Save sample predictions for validation\n",
    "sample_predictions = risk_assessment.head(100)\n",
    "sample_predictions.to_csv(models_dir / \"sample_predictions.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Models and results saved successfully!\")\n",
    "print(f\"üìÅ Location: {models_dir}\")\n",
    "print(f\"üìä Binary model: {best_binary_name} (AUC: {best_binary_model[1]['auc']:.3f})\")\n",
    "print(f\"üìä Multi-class model: {best_multi_name} (F1: {best_multi_model[1]['f1']:.3f})\")\n",
    "print(\"\\nüöÄ Models ready for deployment!\")\n",
    "print(\"üìì Next step: Create intervention recommendation system\")\n",
    "print(\"üì± Then: Build teacher dashboard\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",
   "name": "python3"\n  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}