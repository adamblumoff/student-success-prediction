{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OULAD Data Exploration\n",
    "\n",
    "Initial exploration of the Open University Learning Analytics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pathlib import Path\n\nwarnings.filterwarnings('ignore')\n\n# Set plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\n# Data directory\ndata_dir = Path(\"../data/raw\")\n\n# Load OULAD datasets\nprint(\"Loading OULAD datasets...\")\n\ntry:\n    # Core datasets\n    students = pd.read_csv(data_dir / \"studentInfo.csv\")\n    courses = pd.read_csv(data_dir / \"courses.csv\")\n    assessments = pd.read_csv(data_dir / \"assessments.csv\")\n    student_assessment = pd.read_csv(data_dir / \"studentAssessment.csv\")\n    vle = pd.read_csv(data_dir / \"vle.csv\")\n    student_vle = pd.read_csv(data_dir / \"studentVle.csv\")\n    student_registration = pd.read_csv(data_dir / \"studentRegistration.csv\")\n    \n    print(\"‚úì All datasets loaded successfully!\")\n    print(f\"Students: {len(students):,} records\")\n    print(f\"Courses: {len(courses):,} records\")\n    print(f\"Assessments: {len(assessments):,} records\")\n    print(f\"Student Assessments: {len(student_assessment):,} records\")\n    print(f\"VLE Activities: {len(vle):,} records\")\n    print(f\"Student VLE Interactions: {len(student_vle):,} records\")\n    print(f\"Student Registrations: {len(student_registration):,} records\")\n    \nexcept FileNotFoundError as e:\n    print(f\"‚ùå Error: {e}\")\n    print(\"Please download the OULAD dataset first:\")\n    print(\"Run: python ../src/data/download.py\")"
  },
  {
   "cell_type": "code",
   "source": "# Summary of Key Findings for Predictive Modeling\n\nprint(\"=== KEY INSIGHTS FOR STUDENT SUCCESS PREDICTION ===\")\nprint()\nprint(\"1. EARLY WARNING INDICATORS:\")\nprint(\"   - VLE engagement in first 50 days strongly correlates with outcomes\")\nprint(\"   - Assessment submission patterns predict withdrawal risk\")\nprint(\"   - Early assessment performance (first 100 days) is highly predictive\")\nprint()\nprint(\"2. DEMOGRAPHIC FACTORS:\")\nprint(\"   - Age band and education level show correlation with success\")\nprint(\"   - Gender differences in completion rates\")\nprint(\"   - Study load affects outcomes\")\nprint()\nprint(\"3. BEHAVIORAL PATTERNS:\")\nprint(\"   - Total VLE clicks and active days differ by outcome\")\nprint(\"   - Assessment type performance varies by final result\")\nprint(\"   - Missing submissions are strong withdrawal indicators\")\nprint()\nprint(\"4. TEMPORAL PATTERNS:\")\nprint(\"   - Registration timing affects outcomes\")\nprint(\"   - Engagement patterns change over time\")\nprint(\"   - Critical intervention windows exist\")\nprint()\nprint(\"=== NEXT STEPS FOR PREDICTIVE MODELING ===\")\nprint()\nprint(\"1. FEATURE ENGINEERING:\")\nprint(\"   - Create early engagement metrics (first 3-4 weeks)\")\nprint(\"   - Build assessment performance trajectories\")\nprint(\"   - Calculate submission consistency scores\")\nprint(\"   - Design activity pattern features\")\nprint()\nprint(\"2. MODEL DEVELOPMENT:\")\nprint(\"   - Binary classification: Pass/Fail (exclude withdrawn)\")\nprint(\"   - Multi-class classification: Pass/Fail/Distinction/Withdrawn\")\nprint(\"   - Time-series prediction: Weekly risk assessment\")\nprint(\"   - Intervention timing optimization\")\nprint()\nprint(\"3. EVALUATION METRICS:\")\nprint(\"   - Precision/Recall for at-risk students\")\nprint(\"   - Early prediction accuracy (weeks 3-4)\")\nprint(\"   - Intervention timing effectiveness\")\nprint(\"   - False positive/negative cost analysis\")\n\n# Create summary statistics for modeling\nif 'students' in locals():\n    print(\"\\n=== DATASET SUMMARY FOR MODELING ===\")\n    print(f\"Total student records: {len(students):,}\")\n    print(f\"Success rate: {(students['final_result'].isin(['Pass', 'Distinction']).sum() / len(students) * 100):.1f}%\")\n    print(f\"Failure rate: {(students['final_result'] == 'Fail').sum() / len(students) * 100:.1f}%\")\n    print(f\"Withdrawal rate: {(students['final_result'] == 'Withdrawn').sum() / len(students) * 100:.1f}%\")\n    \n    # Time window analysis\n    if 'student_vle' in locals():\n        print(f\"\\nVLE interaction timeframe: {student_vle['date'].min()} to {student_vle['date'].max()} days\")\n        print(f\"Total VLE interactions: {len(student_vle):,}\")\n        \n    if 'student_assessment' in locals():\n        print(f\"Assessment timeframe: {student_assessment['date_submitted'].min()} to {student_assessment['date_submitted'].max()}\")\n        print(f\"Total assessment submissions: {len(student_assessment):,}\")\n\nprint(\"\\nüéØ Ready for predictive modeling development!\")\nprint(\"üìä Next notebook: 02_feature_engineering.ipynb\")\nprint(\"ü§ñ Then: 03_predictive_modeling.ipynb\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Key Insights and Next Steps\n\nBased on the exploratory analysis, we can identify several key patterns that will inform our predictive modeling:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Assessment Performance Analysis\nif 'student_assessment' in locals() and 'students' in locals() and 'assessments' in locals():\n    # Merge assessment data with student outcomes and assessment details\n    assessment_performance = student_assessment.merge(\n        students[['id_student', 'code_module', 'code_presentation', 'final_result']], \n        on=['id_student', 'code_module', 'code_presentation']\n    ).merge(\n        assessments[['id_assessment', 'assessment_type', 'date', 'weight']], \n        on='id_assessment'\n    )\n    \n    # Remove withdrawn students for assessment analysis\n    assessment_performance = assessment_performance[assessment_performance['final_result'] != 'Withdrawn']\n    \n    print(\"=== ASSESSMENT PERFORMANCE BY OUTCOME ===\")\n    performance_by_outcome = assessment_performance.groupby('final_result')['score'].agg(['mean', 'median', 'std', 'count']).round(2)\n    print(performance_by_outcome)\n    \n    print(\"\\n=== ASSESSMENT PERFORMANCE BY TYPE ===\")\n    performance_by_type = assessment_performance.groupby(['assessment_type', 'final_result'])['score'].mean().unstack()\n    print(performance_by_type.round(2))\n    \n    # Visualize assessment performance\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Score distribution by outcome\n    for outcome in assessment_performance['final_result'].unique():\n        data = assessment_performance[assessment_performance['final_result'] == outcome]['score']\n        axes[0,0].hist(data, alpha=0.7, label=outcome, bins=20)\n    axes[0,0].set_title('Score Distribution by Final Result')\n    axes[0,0].set_xlabel('Score')\n    axes[0,0].set_ylabel('Frequency')\n    axes[0,0].legend()\n    \n    # Box plot of scores by outcome\n    assessment_performance.boxplot(column='score', by='final_result', ax=axes[0,1])\n    axes[0,1].set_title('Score Distribution by Final Result')\n    axes[0,1].set_xlabel('Final Result')\n    axes[0,1].set_ylabel('Score')\n    \n    # Assessment type performance\n    performance_by_type.plot(kind='bar', ax=axes[1,0])\n    axes[1,0].set_title('Average Score by Assessment Type')\n    axes[1,0].set_xlabel('Assessment Type')\n    axes[1,0].set_ylabel('Average Score')\n    axes[1,0].legend(title='Final Result')\n    \n    # Early assessment performance (first 100 days)\n    early_assessments = assessment_performance[assessment_performance['date'] <= 100]\n    early_performance = early_assessments.groupby('final_result')['score'].mean()\n    early_performance.plot(kind='bar', ax=axes[1,1])\n    axes[1,1].set_title('Early Assessment Performance (First 100 Days)')\n    axes[1,1].set_xlabel('Final Result')\n    axes[1,1].set_ylabel('Average Score')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Submission behavior analysis\n    submission_behavior = assessment_performance.groupby(['id_student', 'code_module', 'code_presentation', 'final_result']).agg({\n        'score': ['count', 'mean'],\n        'date_submitted': lambda x: (x.isna()).sum()  # Count missing submissions\n    }).reset_index()\n    \n    submission_behavior.columns = ['id_student', 'code_module', 'code_presentation', 'final_result', 'submitted_count', 'avg_score', 'missing_submissions']\n    \n    print(\"\\n=== SUBMISSION BEHAVIOR BY OUTCOME ===\")\n    submission_by_outcome = submission_behavior.groupby('final_result').agg({\n        'submitted_count': ['mean', 'std'],\n        'avg_score': ['mean', 'std'],\n        'missing_submissions': ['mean', 'std']\n    }).round(2)\n    \n    print(submission_by_outcome)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Assessment Performance Analysis\n\nUnderstanding how students perform on assessments and how this relates to their final outcomes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# VLE Engagement Analysis\nif 'student_vle' in locals() and 'students' in locals():\n    # Merge VLE data with student outcomes\n    vle_engagement = student_vle.merge(students[['id_student', 'code_module', 'code_presentation', 'final_result']], \n                                      on=['id_student', 'code_module', 'code_presentation'])\n    \n    # Calculate engagement metrics per student\n    engagement_metrics = vle_engagement.groupby(['id_student', 'code_module', 'code_presentation', 'final_result']).agg({\n        'sum_click': 'sum',\n        'date': 'count'  # Number of days active\n    }).reset_index()\n    \n    engagement_metrics.columns = ['id_student', 'code_module', 'code_presentation', 'final_result', 'total_clicks', 'active_days']\n    \n    print(\"=== VLE ENGAGEMENT BY OUTCOME ===\")\n    engagement_by_outcome = engagement_metrics.groupby('final_result').agg({\n        'total_clicks': ['mean', 'median', 'std'],\n        'active_days': ['mean', 'median', 'std']\n    }).round(2)\n    \n    print(engagement_by_outcome)\n    \n    # Visualize engagement patterns\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Total clicks by outcome\n    engagement_metrics.boxplot(column='total_clicks', by='final_result', ax=axes[0])\n    axes[0].set_title('Total VLE Clicks by Final Result')\n    axes[0].set_xlabel('Final Result')\n    axes[0].set_ylabel('Total Clicks')\n    \n    # Active days by outcome\n    engagement_metrics.boxplot(column='active_days', by='final_result', ax=axes[1])\n    axes[1].set_title('Active Days by Final Result')\n    axes[1].set_xlabel('Final Result')\n    axes[1].set_ylabel('Active Days')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Early engagement analysis (first 50 days)\n    early_engagement = vle_engagement[vle_engagement['date'] <= 50].groupby(['id_student', 'code_module', 'code_presentation', 'final_result']).agg({\n        'sum_click': 'sum'\n    }).reset_index()\n    \n    print(\"\\n=== EARLY ENGAGEMENT (First 50 Days) ===\")\n    early_by_outcome = early_engagement.groupby('final_result')['sum_click'].agg(['mean', 'median', 'std']).round(2)\n    print(early_by_outcome)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## VLE Engagement Analysis\n\nUnderstanding student engagement patterns through Virtual Learning Environment interactions is crucial for predicting student success.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizations for Student Success Analysis\nif 'students' in locals():\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Final result distribution\n    students['final_result'].value_counts().plot(kind='bar', ax=axes[0,0], \n                                                 color=['green', 'red', 'orange', 'blue'])\n    axes[0,0].set_title('Final Results Distribution')\n    axes[0,0].set_xlabel('Final Result')\n    axes[0,0].set_ylabel('Number of Students')\n    \n    # Gender vs Final Result\n    gender_result = pd.crosstab(students['gender'], students['final_result'])\n    gender_result.plot(kind='bar', ax=axes[0,1], stacked=True)\n    axes[0,1].set_title('Gender vs Final Result')\n    axes[0,1].set_xlabel('Gender')\n    axes[0,1].legend(title='Final Result')\n    \n    # Age band vs Final Result\n    age_result = pd.crosstab(students['age_band'], students['final_result'])\n    age_result.plot(kind='bar', ax=axes[1,0], stacked=True)\n    axes[1,0].set_title('Age Band vs Final Result')\n    axes[1,0].set_xlabel('Age Band')\n    axes[1,0].legend(title='Final Result')\n    \n    # Education level vs Final Result\n    edu_result = pd.crosstab(students['highest_education'], students['final_result'])\n    edu_result.plot(kind='bar', ax=axes[1,1], stacked=True)\n    axes[1,1].set_title('Education Level vs Final Result')\n    axes[1,1].set_xlabel('Education Level')\n    axes[1,1].legend(title='Final Result')\n    \n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Student Information Analysis\nif 'students' in locals():\n    print(\"=== STUDENT DEMOGRAPHICS ===\")\n    print(f\"Total students: {len(students):,}\")\n    print(f\"Unique students: {students['id_student'].nunique():,}\")\n    print(f\"Study periods: {students['code_presentation'].nunique()}\")\n    print(f\"Modules: {students['code_module'].nunique()}\")\n    \n    print(\"\\n=== FINAL RESULTS DISTRIBUTION ===\")\n    print(students['final_result'].value_counts())\n    \n    print(\"\\n=== STUDENT DEMOGRAPHICS ===\")\n    print(f\"Gender distribution:\")\n    print(students['gender'].value_counts())\n    \n    print(f\"\\nAge groups:\")\n    print(students['age_band'].value_counts())\n    \n    print(f\"\\nEducation level:\")\n    print(students['highest_education'].value_counts())\n    \n    print(f\"\\nStudy load:\")\n    print(students['studied_credits'].value_counts())\n    \n    # Display basic info\n    print(\"\\n=== DATASET INFO ===\")\n    print(students.info())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset Overview\n\nThe Open University Learning Analytics Dataset (OULAD) contains data about courses, students and their interactions with Virtual Learning Environment (VLE) for seven selected courses (called modules).\n\n### Key Datasets:\n- **studentInfo.csv**: Student demographics and final results\n- **courses.csv**: Course information \n- **assessments.csv**: Assessment details\n- **studentAssessment.csv**: Student scores on assessments\n- **vle.csv**: Virtual Learning Environment activities\n- **studentVle.csv**: Student interactions with VLE\n- **studentRegistration.csv**: Student registration dates",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}